{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "15baL8IDwybZ",
        "outputId": "16ca3747-4df8-4747-f737-c204f1475348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Greedy Layer-wise Unsupervised Pretraining...\n",
            "Network architecture: Input(784) -> 512 -> 256 -> 128\n",
            "\n",
            "--- Pretraining Layer 1 ---\n",
            "Input dim: 784, Hidden dim: 512\n",
            "Layer 1 pretraining complete. Final loss: 0.6224\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\n",
            "--- Pretraining Layer 2 ---\n",
            "Input dim: 512, Hidden dim: 256\n",
            "Layer 2 pretraining complete. Final loss: 5.9051\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\n",
            "--- Pretraining Layer 3 ---\n",
            "Input dim: 256, Hidden dim: 128\n",
            "Layer 3 pretraining complete. Final loss: 33.6777\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\n",
            "--- Building Deep Network with Pretrained Weights ---\n",
            "\n",
            "Pretrained model summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"pretrained_deep_network\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"pretrained_deep_network\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_11 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pretrained_layer_1 (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pretrained_layer_2 (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pretrained_layer_3 (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pretrained_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pretrained_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ pretrained_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m566,144\u001b[0m (2.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">566,144</span> (2.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m566,144\u001b[0m (2.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">566,144</span> (2.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fine-tuning on Supervised Task ---\n",
            "Phase 1: Training output layer only (frozen pretrained layers)\n",
            "Epoch 1/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1029 - loss: 19.2068 - val_accuracy: 0.0850 - val_loss: 6.9858\n",
            "Epoch 2/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1153 - loss: 6.6591 - val_accuracy: 0.0800 - val_loss: 4.0946\n",
            "Epoch 3/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1141 - loss: 4.2997 - val_accuracy: 0.0800 - val_loss: 3.8721\n",
            "Epoch 4/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0990 - loss: 4.1426 - val_accuracy: 0.0850 - val_loss: 3.7541\n",
            "Epoch 5/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.1070 - loss: 4.2361 - val_accuracy: 0.0850 - val_loss: 3.7305\n",
            "Epoch 6/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.1170 - loss: 4.0856 - val_accuracy: 0.0900 - val_loss: 3.6600\n",
            "Epoch 7/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.1040 - loss: 3.9585 - val_accuracy: 0.0800 - val_loss: 3.6682\n",
            "Epoch 8/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0928 - loss: 4.0930 - val_accuracy: 0.1100 - val_loss: 3.7283\n",
            "Epoch 9/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1132 - loss: 4.0208 - val_accuracy: 0.0850 - val_loss: 3.6752\n",
            "Epoch 10/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1187 - loss: 3.8751 - val_accuracy: 0.0900 - val_loss: 3.5425\n",
            "Epoch 11/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0981 - loss: 3.6952 - val_accuracy: 0.0900 - val_loss: 3.5277\n",
            "Epoch 12/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1208 - loss: 3.7973 - val_accuracy: 0.0800 - val_loss: 3.5349\n",
            "Epoch 13/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1118 - loss: 3.7785 - val_accuracy: 0.0850 - val_loss: 3.5616\n",
            "Epoch 14/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1242 - loss: 3.7741 - val_accuracy: 0.0900 - val_loss: 3.5748\n",
            "Epoch 15/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1178 - loss: 3.6123 - val_accuracy: 0.0800 - val_loss: 3.4547\n",
            "Epoch 16/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1174 - loss: 3.5198 - val_accuracy: 0.0950 - val_loss: 3.3825\n",
            "Epoch 17/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1149 - loss: 3.4714 - val_accuracy: 0.0800 - val_loss: 3.4683\n",
            "Epoch 18/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1296 - loss: 3.3760 - val_accuracy: 0.0750 - val_loss: 3.4868\n",
            "Epoch 19/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1372 - loss: 3.4176 - val_accuracy: 0.0900 - val_loss: 3.3292\n",
            "Epoch 20/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1224 - loss: 3.3095 - val_accuracy: 0.0900 - val_loss: 3.2600\n",
            "Epoch 21/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1488 - loss: 3.3555 - val_accuracy: 0.1000 - val_loss: 3.2563\n",
            "Epoch 22/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1307 - loss: 3.2827 - val_accuracy: 0.0950 - val_loss: 3.1921\n",
            "Epoch 23/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1274 - loss: 3.2459 - val_accuracy: 0.0850 - val_loss: 3.2567\n",
            "Epoch 24/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.1172 - loss: 3.3046 - val_accuracy: 0.1000 - val_loss: 3.2777\n",
            "Epoch 25/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1380 - loss: 3.2676 - val_accuracy: 0.0850 - val_loss: 3.2776\n",
            "\n",
            "Phase 2: Unfreezing all layers and fine-tuning entire network\n",
            "Epoch 1/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.1435 - loss: 3.0574 - val_accuracy: 0.1100 - val_loss: 3.1946\n",
            "Epoch 2/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.2218 - loss: 2.6173 - val_accuracy: 0.1050 - val_loss: 3.0642\n",
            "Epoch 3/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.2581 - loss: 2.3264 - val_accuracy: 0.0850 - val_loss: 3.0703\n",
            "Epoch 4/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.3226 - loss: 2.0546 - val_accuracy: 0.1000 - val_loss: 3.0829\n",
            "Epoch 5/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4242 - loss: 1.7370 - val_accuracy: 0.1000 - val_loss: 3.0613\n",
            "Epoch 6/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4981 - loss: 1.5500 - val_accuracy: 0.0750 - val_loss: 3.0844\n",
            "Epoch 7/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5711 - loss: 1.3234 - val_accuracy: 0.0900 - val_loss: 3.0640\n",
            "Epoch 8/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.6654 - loss: 1.1381 - val_accuracy: 0.1050 - val_loss: 3.1069\n",
            "Epoch 9/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7242 - loss: 1.0599 - val_accuracy: 0.0850 - val_loss: 3.0985\n",
            "Epoch 10/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8142 - loss: 0.8658 - val_accuracy: 0.1150 - val_loss: 3.1166\n",
            "Epoch 11/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8333 - loss: 0.7930 - val_accuracy: 0.1100 - val_loss: 3.1203\n",
            "Epoch 12/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9290 - loss: 0.6182 - val_accuracy: 0.0900 - val_loss: 3.1240\n",
            "Epoch 13/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9377 - loss: 0.5587 - val_accuracy: 0.1050 - val_loss: 3.1553\n",
            "Epoch 14/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9614 - loss: 0.4848 - val_accuracy: 0.0850 - val_loss: 3.1802\n",
            "Epoch 15/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9666 - loss: 0.4260 - val_accuracy: 0.1000 - val_loss: 3.2380\n",
            "Epoch 16/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9924 - loss: 0.3507 - val_accuracy: 0.1000 - val_loss: 3.2398\n",
            "Epoch 17/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9880 - loss: 0.3122 - val_accuracy: 0.0950 - val_loss: 3.2656\n",
            "Epoch 18/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9944 - loss: 0.2718 - val_accuracy: 0.0950 - val_loss: 3.3013\n",
            "Epoch 19/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9952 - loss: 0.2303 - val_accuracy: 0.0950 - val_loss: 3.3572\n",
            "Epoch 20/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.2146 - val_accuracy: 0.1050 - val_loss: 3.3550\n",
            "Epoch 21/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.1881 - val_accuracy: 0.0950 - val_loss: 3.3505\n",
            "Epoch 22/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.1611 - val_accuracy: 0.1100 - val_loss: 3.3731\n",
            "Epoch 23/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.1464 - val_accuracy: 0.1050 - val_loss: 3.4192\n",
            "Epoch 24/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.1290 - val_accuracy: 0.1050 - val_loss: 3.4187\n",
            "Epoch 25/25\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.1163 - val_accuracy: 0.1050 - val_loss: 3.4383\n",
            "\n",
            "--- Pretraining Protocol Complete ---\n",
            "Final training accuracy: 1.0000\n",
            "Final validation accuracy: 0.1050\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPSEUDO-CODE VERSION:\\n\\nfunction GreedyLayerwisePretraining(X_train, layer_dimensions):\\n    X_normalized = Normalize(X_train)  # standardize input\\n    pretrained_weights = []\\n    current_input = X_normalized\\n    \\n    # Phase 1: Layer-wise pretraining\\n    for each layer_dim in layer_dimensions:\\n        # Create autoencoder for current layer\\n        autoencoder = CreateAutoencoder(input_dim=current_input.shape, \\n                                       hidden_dim=layer_dim)\\n        \\n        # Train autoencoder unsupervised (reconstruction task)\\n        autoencoder.train(input=current_input, target=current_input)\\n        \\n        # Extract encoder portion and weights\\n        encoder = autoencoder.get_encoder()\\n        weights = encoder.get_weights()\\n        pretrained_weights.append(weights)\\n        \\n        # Transform data through encoder for next layer\\n        current_input = encoder.transform(current_input)\\n    \\n    # Phase 2: Build deep network\\n    deep_network = CreateEmptyNetwork()\\n    for layer_idx, weights in enumerate(pretrained_weights):\\n        layer = deep_network.add_layer(size=layer_dimensions[layer_idx])\\n        layer.set_weights(weights)\\n    \\n    # Phase 3: Supervised fine-tuning (if needed)\\n    deep_network.add_output_layer(num_classes)\\n    \\n    # Two-phase fine-tuning to avoid recompilation issues\\n    deep_network.train_frozen(X_normalized, y_train, epochs=25)\\n    deep_network.unfreeze_all_layers()\\n    deep_network.train_unfrozen(X_normalized, y_train, epochs=25, lower_lr=True)\\n    \\n    return deep_network\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\"\"\"\n",
        "Programming Exercise 3: Unsupervised Pretraining\n",
        "MSDS-534-B01: Deep Learning\n",
        "Pranav Arora\n",
        "August 10, 2025\n",
        "\n",
        "Implementation of Greedy Layer-wise Unsupervised Pretraining Protocol\n",
        "Based on concepts from Goodfellow et al. (2016), Deep Learning, Section 15.1\n",
        "\n",
        "Note: Fixed recompilation issue during training by splitting into two phases\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def create_autoencoder_layer(input_dim, encoding_dim, activation='relu'):\n",
        "    \"\"\"\n",
        "    Creates a single autoencoder for pretraining one layer\n",
        "    \"\"\"\n",
        "    # encoder part\n",
        "    encoder_input = layers.Input(shape=(input_dim,))\n",
        "    encoded = layers.Dense(encoding_dim, activation=activation)(encoder_input)\n",
        "\n",
        "    # decoder part\n",
        "    decoded = layers.Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "    # full autoencoder model\n",
        "    autoencoder = Model(encoder_input, decoded)\n",
        "\n",
        "    # separate encoder model for later use\n",
        "    encoder = Model(encoder_input, encoded)\n",
        "\n",
        "    return autoencoder, encoder\n",
        "\n",
        "def greedy_layerwise_pretraining(X_train, layer_dims, epochs_per_layer=50, batch_size=32):\n",
        "    \"\"\"\n",
        "    Implements greedy layer-wise unsupervised pretraining protocol\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : numpy array\n",
        "        Raw input training data\n",
        "    layer_dims : list\n",
        "        List of dimensions for each hidden layer\n",
        "    epochs_per_layer : int\n",
        "        Number of epochs to train each layer\n",
        "    batch_size : int\n",
        "        Batch size for training\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pretrained_model : keras Model\n",
        "        The pretrained deep network\n",
        "    layer_weights : list\n",
        "        List of pretrained weights for each layer\n",
        "    X_normalized : numpy array\n",
        "        Normalized training data (needed for fine-tuning)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Starting Greedy Layer-wise Unsupervised Pretraining...\")\n",
        "    print(f\"Network architecture: Input({X_train.shape[1]}) -> {' -> '.join(map(str, layer_dims))}\")\n",
        "\n",
        "    # normalize input data (important for stable training)\n",
        "    scaler = StandardScaler()\n",
        "    X_normalized = scaler.fit_transform(X_train)\n",
        "\n",
        "    # store pretrained weights for each layer\n",
        "    pretrained_weights = []\n",
        "    pretrained_biases = []\n",
        "\n",
        "    # current input for training (starts with raw data)\n",
        "    current_input = X_normalized\n",
        "    input_dim = X_train.shape[1]\n",
        "\n",
        "    # Step 1: Greedy layer-wise pretraining\n",
        "    for layer_idx, hidden_dim in enumerate(layer_dims):\n",
        "        print(f\"\\n--- Pretraining Layer {layer_idx + 1} ---\")\n",
        "        print(f\"Input dim: {input_dim}, Hidden dim: {hidden_dim}\")\n",
        "\n",
        "        # create autoencoder for this layer\n",
        "        autoencoder, encoder = create_autoencoder_layer(input_dim, hidden_dim)\n",
        "\n",
        "        # compile with reconstruction loss\n",
        "        autoencoder.compile(\n",
        "            optimizer='adam',\n",
        "            loss='mse'  # reconstruction error\n",
        "        )\n",
        "\n",
        "        # train this layer's autoencoder\n",
        "        history = autoencoder.fit(\n",
        "            current_input, current_input,  # unsupervised: input=target\n",
        "            epochs=epochs_per_layer,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0,\n",
        "            validation_split=0.1\n",
        "        )\n",
        "\n",
        "        # extract and save the encoder weights (what we actually want)\n",
        "        encoder_weights = encoder.layers[1].get_weights()\n",
        "        pretrained_weights.append(encoder_weights[0])  # weight matrix\n",
        "        pretrained_biases.append(encoder_weights[1])   # bias vector\n",
        "\n",
        "        print(f\"Layer {layer_idx + 1} pretraining complete. Final loss: {history.history['loss'][-1]:.4f}\")\n",
        "\n",
        "        # transform current input through the encoder for next layer\n",
        "        current_input = encoder.predict(current_input)\n",
        "        input_dim = hidden_dim\n",
        "\n",
        "    # Step 2: Build the full deep network with pretrained weights\n",
        "    print(\"\\n--- Building Deep Network with Pretrained Weights ---\")\n",
        "\n",
        "    # create the deep network\n",
        "    model_input = layers.Input(shape=(X_train.shape[1],))\n",
        "    x = model_input\n",
        "\n",
        "    # add each layer with pretrained weights\n",
        "    for layer_idx, hidden_dim in enumerate(layer_dims):\n",
        "        layer = layers.Dense(\n",
        "            hidden_dim,\n",
        "            activation='relu',\n",
        "            name=f'pretrained_layer_{layer_idx + 1}'\n",
        "        )\n",
        "        x = layer(x)\n",
        "\n",
        "        # set the pretrained weights\n",
        "        layer.set_weights([pretrained_weights[layer_idx], pretrained_biases[layer_idx]])\n",
        "\n",
        "    # create the pretrained model (no output layer yet - depends on task)\n",
        "    pretrained_model = Model(model_input, x, name='pretrained_deep_network')\n",
        "\n",
        "    return pretrained_model, (pretrained_weights, pretrained_biases), X_normalized\n",
        "\n",
        "def add_supervised_output_layer(pretrained_model, num_classes, freeze_pretrained=True):\n",
        "    \"\"\"\n",
        "    Adds supervised output layer for fine-tuning\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    pretrained_model : keras Model\n",
        "        The pretrained network\n",
        "    num_classes : int\n",
        "        Number of output classes (or 1 for regression)\n",
        "    freeze_pretrained : bool\n",
        "        Whether to freeze pretrained layers during initial fine-tuning\n",
        "    \"\"\"\n",
        "\n",
        "    # freeze pretrained layers if specified\n",
        "    if freeze_pretrained:\n",
        "        for layer in pretrained_model.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "    # add output layer for supervised task\n",
        "    output = layers.Dense(num_classes, activation='softmax')(pretrained_model.output)\n",
        "\n",
        "    # create final model\n",
        "    final_model = Model(pretrained_model.input, output)\n",
        "\n",
        "    return final_model\n",
        "\n",
        "def fine_tune_network(model, X_train, y_train, epochs=50, unfreeze_after=25):\n",
        "    \"\"\"\n",
        "    Fine-tunes the entire network on supervised task\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : keras Model\n",
        "        Model with pretrained weights and output layer\n",
        "    X_train : numpy array\n",
        "        Training data\n",
        "    y_train : numpy array\n",
        "        Training labels\n",
        "    epochs : int\n",
        "        Total epochs for fine-tuning\n",
        "    unfreeze_after : int\n",
        "        Epoch after which to unfreeze all layers\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n--- Fine-tuning on Supervised Task ---\")\n",
        "\n",
        "    # Phase 1: Train with frozen pretrained layers\n",
        "    print(\"Phase 1: Training output layer only (frozen pretrained layers)\")\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    history1 = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=unfreeze_after,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Phase 2: Unfreeze and train all layers with lower learning rate\n",
        "    print(f\"\\nPhase 2: Unfreezing all layers and fine-tuning entire network\")\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # recompile with lower learning rate for fine-tuning\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    history2 = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs - unfreeze_after,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # combine histories\n",
        "    history = {\n",
        "        'loss': history1.history['loss'] + history2.history['loss'],\n",
        "        'accuracy': history1.history['accuracy'] + history2.history['accuracy'],\n",
        "        'val_loss': history1.history['val_loss'] + history2.history['val_loss'],\n",
        "        'val_accuracy': history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
        "    }\n",
        "\n",
        "    # create a simple object to hold the combined history\n",
        "    class CombinedHistory:\n",
        "        def __init__(self, history_dict):\n",
        "            self.history = history_dict\n",
        "\n",
        "    return model, CombinedHistory(history)\n",
        "\n",
        "# Alternative implementation using learning rate scheduler (more stable)\n",
        "def fine_tune_network_with_scheduler(model, X_train, y_train, epochs=50, unfreeze_after=25):\n",
        "    \"\"\"\n",
        "    Alternative fine-tuning with learning rate scheduler (avoids recompilation issues)\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Fine-tuning on Supervised Task (with LR scheduler) ---\")\n",
        "\n",
        "    # unfreeze layers based on epochs\n",
        "    def selective_unfreeze(epoch):\n",
        "        if epoch == unfreeze_after:\n",
        "            print(f\"\\nUnfreezing all layers at epoch {epoch}\")\n",
        "            for layer in model.layers:\n",
        "                layer.trainable = True\n",
        "\n",
        "    # learning rate schedule\n",
        "    def lr_schedule(epoch):\n",
        "        if epoch < unfreeze_after:\n",
        "            return 0.001  # higher lr for output layer only\n",
        "        else:\n",
        "            return 0.0001  # lower lr for full network fine-tuning\n",
        "\n",
        "    # callbacks\n",
        "    lr_scheduler = keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "    unfreeze_callback = keras.callbacks.LambdaCallback(\n",
        "        on_epoch_begin=lambda epoch, logs: selective_unfreeze(epoch)\n",
        "    )\n",
        "\n",
        "    # compile once\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # train with callbacks\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[lr_scheduler, unfreeze_callback],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# ============ MAIN EXECUTION EXAMPLE ============\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # simulate some data\n",
        "    np.random.seed(42)\n",
        "    n_samples = 1000\n",
        "    n_features = 784  # like MNIST\n",
        "    n_classes = 10\n",
        "\n",
        "    # generate dummy data\n",
        "    X_train = np.random.randn(n_samples, n_features)\n",
        "    y_train = np.random.randint(0, n_classes, n_samples)\n",
        "\n",
        "    # define architecture for pretraining\n",
        "    hidden_layers = [512, 256, 128]  # progressively smaller layers\n",
        "\n",
        "    # Step 1: Unsupervised pretraining (returns normalized data too)\n",
        "    pretrained_model, weights, X_train_normalized = greedy_layerwise_pretraining(\n",
        "        X_train,\n",
        "        hidden_layers,\n",
        "        epochs_per_layer=30\n",
        "    )\n",
        "\n",
        "    print(\"\\nPretrained model summary:\")\n",
        "    pretrained_model.summary()\n",
        "\n",
        "    # Step 2: Add supervised output layer\n",
        "    final_model = add_supervised_output_layer(\n",
        "        pretrained_model,\n",
        "        num_classes=n_classes,\n",
        "        freeze_pretrained=True\n",
        "    )\n",
        "\n",
        "    # Step 3: Fine-tune on supervised task\n",
        "    # Use the two-phase approach (more stable than recompiling mid-training)\n",
        "    trained_model, history = fine_tune_network(\n",
        "        final_model,\n",
        "        X_train_normalized,  # use normalized data\n",
        "        y_train,\n",
        "        epochs=50,\n",
        "        unfreeze_after=25\n",
        "    )\n",
        "\n",
        "    # Alternative: use scheduler version if you prefer single training run\n",
        "    # trained_model, history = fine_tune_network_with_scheduler(\n",
        "    #     final_model,\n",
        "    #     X_train_normalized,  # use normalized data\n",
        "    #     y_train,\n",
        "    #     epochs=50,\n",
        "    #     unfreeze_after=25\n",
        "    # )\n",
        "\n",
        "    print(\"\\n--- Pretraining Protocol Complete ---\")\n",
        "    print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
        "    print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n"
      ]
    }
  ]
}